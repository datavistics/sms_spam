{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "## Understanding the data\n",
    "This data is quite messy. Given that this is spam classification over other tasks like say topic classification, we need to adjust our approach accordingly. When we see odd punctuation, misspelled words, erratic patterns, odd capitializations, these are usually features and not noise.\n",
    "\n",
    "We also have a data set of size 5572, this is rather small for advanced methods like Deep Learning, and we **might** result in an overfit model. Because of this, I will spend my time on more traditional methods, but I will look into it if I have time.\n",
    "\n",
    "## Goal\n",
    "The [paper](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf) that comes with the data set, boasts \n",
    "\n",
    "| Classifier          | SC%    | BH%  | Acc%   | MCC   |\n",
    "|---------------------|--------|------|--------|-------|\n",
    "| SVM + tok1          | 83.10  | 0.18 | 97.64  | 0.893 |\n",
    "| Boosted NB + tok2   | 84.48  | 0.53 | 97.50  | 0.887 |\n",
    "| Boosted C4.5 + tok2 | 82.91  | 0.29 | 97.50  | 0.887 |\n",
    "\n",
    "# Status\n",
    "By the time the data gets to this notebook, it has:\n",
    "* Been downloaded, and unzipped\n",
    "* Read into a pandas dataframe\n",
    "\n",
    "# Approach\n",
    "There are a few factors at play here and we want to optimize each step.\n",
    "## Tokenization\n",
    "\n",
    "\n",
    "## Modeling\n",
    "### Model Choice\n",
    "### Hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.166542Z",
     "start_time": "2018-01-23T17:54:07.289065Z"
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir/'src'))\n",
    "\n",
    "# These are utilities that I created to reduce notebook clutter\n",
    "from make_dataframe import make_dataframe, master_data_handler\n",
    "from utilities import scrm114_tokenizer, eager_split_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get our data, and then we encode it for use with our Classifiers. We should note our mapping for later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.200574Z",
     "start_time": "2018-01-23T17:54:08.168552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data_handler()\n",
    "df = make_dataframe()\n",
    "df.label = df.label.map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its interesting to note that, with no machine learning we can get 86.6% Accuracy! This is a good warning that accuracy will probably not be a good metric for us. It also tells us that we have an unbalanced data set. Depending on our classifier we need to handle this accordingly. \n",
    "\n",
    "SVM has a [class_weight parameter](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py) that allows us to compensate.\n",
    "\n",
    "Naive Bayes has a [complement class](http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) structure that allows us to compensate. It has a corresponding function in sklearn - [ComplementNB](http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.ComplementNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.288311Z",
     "start_time": "2018-01-23T17:54:08.202076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of ham to total:  0.8659368269921034\n"
     ]
    }
   ],
   "source": [
    "ratio = sum(df.label == 0)/len(df.label)\n",
    "print('Ratio of ham to total: ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, we need to choose a tokenizer that will capture the features we want: \n",
    "\n",
    "* Punctuation preservation\n",
    "* Case preservation\n",
    "* Full words in tokens\n",
    "\n",
    "There is research in spam classification [section 3.2](http://www.siefkes.net/ie/winnow-spam.pdf) with some good options. Unfortunately these are all in PCRE regex which python does not support fully (`\\p{Z}` among other atoms). But luckily they aren't too hard to convert, given our data set (minimal control characters). \n",
    "\n",
    "I chose the simplified CRM114. Look how it handles an elipsis, we capture an extra feature of `'..'`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.418106Z",
     "start_time": "2018-01-23T17:54:08.292314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point,',\n",
       " 'crazy.',\n",
       " '.',\n",
       " 'Available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet.',\n",
       " '..',\n",
       " 'Cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat.',\n",
       " '..']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.text[0])\n",
    "scrm114_tokenizer(df.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:08:44.835035Z",
     "start_time": "2018-01-23T18:08:44.820014Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ComplementNB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8779af34828d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ComplementNB'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.719273Z",
     "start_time": "2018-01-23T17:54:08.712769Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.label, test_size=0.65, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:54:08.802211Z",
     "start_time": "2018-01-23T17:54:08.720775Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=scrm114_tokenizer)),\n",
    "#     ('clf', MultinomialNB()),\n",
    "    ('clf', SVC(kernel='linear')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:01:24.742670Z",
     "start_time": "2018-01-23T17:54:08.803725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 729 out of 729 | elapsed:  7.3min finished\n"
     ]
    }
   ],
   "source": [
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "parameters_svm = {\n",
    "    'clf__C': [.7, .08, .9],\n",
    "    'clf__class_weight': [{0: w * ratio} for w in [.192, .1925, .193]],\n",
    "    'clf__gamma': [0.002, 0.003, 0.004],\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vect__max_df': (.03, .1, .3),}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(pipeline_svm, parameters_svm,\n",
    "                          verbose=1, scoring=mcc_scorer)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:01:24.752678Z",
     "start_time": "2018-01-23T18:01:24.744671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8962739070169965\n",
      "Best params:  {'clf__C': 0.08, 'clf__class_weight': {0: 0.16625987078248386}, 'clf__gamma': 0.002, 'vect__max_df': 0.1, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "print('Best score: ', gs_clf_svm.best_score_)\n",
    "print('Best params: ', gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:01:25.184996Z",
     "start_time": "2018-01-23T18:01:24.756682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3126   19]\n",
      " [  60  417]]\n"
     ]
    }
   ],
   "source": [
    "y_true_svm, y_pred_svm = y_test, gs_clf_svm.predict(X_test)\n",
    "print(confusion_matrix(y_true_svm, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:01:25.191992Z",
     "start_time": "2018-01-23T18:01:25.186987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.99      0.99      3145\n",
      "       spam       0.96      0.87      0.91       477\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_svm, y_pred_svm, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:01:25.331233Z",
     "start_time": "2018-01-23T18:01:25.193995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9022136837194339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true_svm, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:34:50.056221Z",
     "start_time": "2018-01-23T18:34:50.048716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8615384615384616 0.13846153846153847\n"
     ]
    }
   ],
   "source": [
    "prior_0 = sum(y_train==0)/len(y_train)\n",
    "prior_1 = sum(y_train==1)/len(y_train)\n",
    "print(prior_0, prior_1)\n",
    "\n",
    "pipeline_mnb = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "#     ('clf', MultinomialNB(class_prior = [prior_0, prior_1])),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:15.800007Z",
     "start_time": "2018-01-23T18:48:03.948565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed:   11.6s finished\n"
     ]
    }
   ],
   "source": [
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': [.17, .2, .23],\n",
    "    'vect__tokenizer': [scrm114_tokenizer, eager_split_tokenizer],\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'vect__max_df': (.4, .5, .6),}\n",
    "\n",
    "gs_clf_mnb = GridSearchCV(pipeline_mnb, parameters_mnb,\n",
    "                          verbose=1, scoring=mcc_scorer)\n",
    "gs_clf_mnb = gs_clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:15.807015Z",
     "start_time": "2018-01-23T18:48:15.801512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.9366999215864544\n",
      "Best params:  {'clf__alpha': 0.17, 'vect__max_df': 0.4, 'vect__ngram_range': (1, 2), 'vect__tokenizer': <function scrm114_tokenizer at 0x000001AB1175B7B8>}\n"
     ]
    }
   ],
   "source": [
    "print('Best score: ', gs_clf_mnb.best_score_)\n",
    "print('Best params: ', gs_clf_mnb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:16.051204Z",
     "start_time": "2018-01-23T18:48:15.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3123   22]\n",
      " [  30  447]]\n"
     ]
    }
   ],
   "source": [
    "y_true_mnb, y_pred_mnb = y_test, gs_clf_mnb.predict(X_test)\n",
    "print(confusion_matrix(y_true_mnb, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:28.123678Z",
     "start_time": "2018-01-23T18:48:28.116671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      3145\n",
      "       spam       0.95      0.94      0.95       477\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_mnb, y_pred_mnb, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:28.847994Z",
     "start_time": "2018-01-23T18:48:28.838484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368201198559103"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true_mnb, y_pred_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:48:28.847994Z",
     "start_time": "2018-01-23T18:48:28.838484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473872018496066"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true_mnb, y_pred_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:24:52.420427Z",
     "start_time": "2018-01-23T18:24:52.403415Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5c93f8407974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_ds\\lib\\site-packages\\bayes\\classifiers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcnb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnnb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNegationNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUniversalSetNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msnb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectiveNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#from lwnb import LocallyWeightedNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cnb'"
     ]
    }
   ],
   "source": [
    "from bayes.classifiers import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:24.450081Z",
     "start_time": "2018-01-23T18:05:24.446079Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_mnb = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=scrm114_tokenizer)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:30.652089Z",
     "start_time": "2018-01-23T18:05:26.244384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "parameters_mnb = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vect__max_df': (.03, .1, .3),}\n",
    "\n",
    "gs_clf_mnb = GridSearchCV(pipeline_mnb, parameters_mnb,\n",
    "                          verbose=1, scoring=mcc_scorer)\n",
    "gs_clf_mnb = gs_clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:30.662095Z",
     "start_time": "2018-01-23T18:05:30.655590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.9191354027879277\n",
      "Best params:  {'vect__max_df': 0.03, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print('Best score: ', gs_clf_mnb.best_score_)\n",
    "print('Best params: ', gs_clf_mnb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:30.925957Z",
     "start_time": "2018-01-23T18:05:30.665099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3135   10]\n",
      " [  33  444]]\n"
     ]
    }
   ],
   "source": [
    "y_true_mnb, y_pred_mnb = y_test, gs_clf_mnb.predict(X_test)\n",
    "print(confusion_matrix(y_true_mnb, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:30.934481Z",
     "start_time": "2018-01-23T18:05:30.928959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99      3145\n",
      "       spam       0.98      0.93      0.95       477\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_mnb, y_pred_mnb, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:05:31.033051Z",
     "start_time": "2018-01-23T18:05:30.940470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473872018496066"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true_mnb, y_pred_mnb)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
